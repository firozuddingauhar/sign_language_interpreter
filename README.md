# sign_language_interpreter
HackJMI project
TEAM PARADOX 
•	Firoz Uddin Gauhar 
•	Mohammad Ahmadullah Khan

We are 1st year computer science students from Jamia Millia Islamia ,Faculty of Engineering and Tecnology , and we writing to provide context and explanation regarding our project for the hackathon(HackJMI). Our team attempted to create a sign recognition model using Python, TensorFlow, and Flutter. The aim is to create an application which will help mute people to convey their emotions efficiently to a person who do not know sign language. it will change their perception for the world and will enhance the productivity of the person using it. The app will also assist it’s user to translate texts into sign language using animations.

Due to time constraints and the project's scale, we were regrettably unable to finish the project as anticipated. Despite these challenges, we were still able to demonstrate our understanding of the technologies involved and our ability to work with multiple programming languages.
 While we were able to acquire the necessary dataset for training TensorFlow, we encountered difficulties in actually commencing the training process. We downloaded the official repository of tensorflow from github. The collected dataset was divided into two parts ,’train ‘and ‘test’.( /Tensorflow/workspace/images/train) and (/Tensorflow/workspace/images/test)
We created short steps for completing our project but we couldn’t  get further from making .the TFrecords.

However, we were able to complete the basic frontend app using Flutter and demonstrate our understanding of the technology. We understand that not being able to complete the full project is a disappointment, but we hope that you can appreciate the effort and dedication that went into our work thus far.

We wanted to provide further explanation regarding our sign recognition model project. The idea behind our project was to train a TensorFlow model on a given dataset of sign language gestures, so that it could then be used to detect and interpret signs in real-time.

The process of training a TensorFlow model involves feeding the model a large number of example gestures, along with their corresponding labels, so that the model can learn to recognize the signs on its own. The training data was to be collected from the data set we acquired.

Once the model was trained, we intended to integrate it into a frontend app built using Flutter. The app would use the trained model to detect and interpret the signs performed by a user in real-time, allowing for a seamless sign language interpretation experience.

While we were unable to complete the full project due to time constraints and the scope of the project, we hope that you can appreciate the potential impact that this technology could have in improving communication for those who use sign language.

Thank you for your understanding and consideration.
